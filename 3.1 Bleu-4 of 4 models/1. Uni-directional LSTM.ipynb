{"cells":[{"cell_type":"markdown","metadata":{"id":"NLjpbXW5DRBB"},"source":["# Seq2Seq model and Evaluation metric - Machine Translation"]},{"cell_type":"markdown","metadata":{"id":"4yMxWc16DRBE"},"source":["### Tutorial Topics\n","- Machine Translation:\n","    - Seq2Seq model\n","    - Evaluation metric\n","\n","### Software Requirements\n","- Python (>=3.6)\n","- PyTorch (>=1.2.0) \n","- Jupyter (latest)\n","- torchtext\n","- NLTK"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoUOTMQF4_QQ","outputId":"776ba5e0-b752-4a9d-c16f-dc91206607af","executionInfo":{"status":"ok","timestamp":1678429363354,"user_tz":480,"elapsed":27892,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"_4a1pOgPENwx"},"source":["## Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"tk6h6fp2DRBP"},"source":["In this tutorial, we will introduce a neural network to translate French sentence to English sentence.\n","\n","We will introduce a important architecture in machine translation: [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence (e.g., sentence) to another. An encoder network condenses an input sequence into a **single vector**, and a decoder network unfolds that vector into a new sequence in target language."]},{"cell_type":"markdown","metadata":{"id":"GpeU8nufDRBP"},"source":["# Sequence to Sequence Learning\n","\n","A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **`encoder`** and **`decoder`**. The `encoder` reads an input sequence one token at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. In classification task, we use this **context** vector as the \"summarization\" of input sequence. In seq2seq model, the decoder uses this context vector as the initial state to generate translation. We will discuss the details in the later section.  \n","\n","![](https://i.imgur.com/tVtHhNp.png)\n","\n"," Picture Courtesy: https://i.imgur.com/tVtHhNp.png\n"," \n","When using a single RNN, there is a one-to-one relationship between `inputs` and `outputs`. But there are not directly one-to-one relationship between source language and target language. \n","\n","Consider a simple sentence \"`Je ne suis pas le chat noir\"` &rarr; \"`I am not the black cat`\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n","\n","With the seq2seq model, by encoding many source inputs into one vector, and decoding from one vector into many target outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector which is a $N$ dimensional representation. In an ideal case, this vector can be considered as the `\"summarization\"` of the sequence.\n","\n","The flow of rest of this tutorial is as follows:\n","1. Preparing data\n","2. Encoder\n","3. Decoder\n","4. Seq2seq\n","5. Training the model\n","6. Loading the trained model checkpoint\n","7. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"L9VR524FDRBQ"},"source":["### Required imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JngouI_r0C0D","outputId":"ce4b85f2-6f3b-40eb-911d-5db3b4345ffe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.8/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.8/dist-packages (0.9.0+cu111)\n","Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.8/dist-packages (0.8.0)\n","Requirement already satisfied: torchtext==0.9.0 in /usr/local/lib/python3.8/dist-packages (0.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.25.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n"]}],"source":["!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AK5A24eUrqwU","outputId":"3d5ab8fd-48ce-4c4c-bc10-b374c10f2d60"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.5.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"]}],"source":["!pip install --upgrade spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D5ocUsBDRBR"},"outputs":[],"source":["import unicodedata\n","import string\n","import re\n","import random\n","import time\n","import datetime\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n","import torchtext\n","# from torchtext.legacy import data\n","import spacy\n","import numpy as np"]},{"cell_type":"code","source":["torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"dZwmlZ-cMmr4","executionInfo":{"status":"ok","timestamp":1678429557340,"user_tz":480,"elapsed":517,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}},"outputId":"f626f8b0-b777-4ac2-a2a8-d57f42a4a9ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.13.1+cu116'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"GxjuYtN0DRBU"},"source":["Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. \n","\n","If you don't have a GPU, set this as CPU. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZONroUKDRBV","outputId":"6bfde7ea-94fc-44f1-b3bd-140d66adeba6","executionInfo":{"status":"ok","timestamp":1678429563965,"user_tz":480,"elapsed":349,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"QmhYeZCdDRBY"},"source":["## 1. Preparing Data"]},{"cell_type":"markdown","metadata":{"id":"nh7pBWq_DRBa"},"source":["***Define tokenizers:***\n","we create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens.\n","\n","`spaCy` has model for each language (\"fr\" for French and \"en\" for English) which need to be loaded so we can access the tokenizer of each model."]},{"cell_type":"markdown","metadata":{"id":"O9DyXb3eDRBa"},"source":["***Note***: the models must first be downloaded using the following on the command line:\n","\n","```\n","python -m spacy download en_core_web_sm\n","python -m spacy download fr_core_news_sm\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQbDo_trz-yX","outputId":"9f966428-9018-4bec-c614-2e408693f70b","executionInfo":{"status":"ok","timestamp":1678429585201,"user_tz":480,"elapsed":14262,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('fr_core_news_sm')\n"]}],"source":["import spacy.cli\n","\n","spacy.cli.download(\"en_core_web_sm\")\n","spacy.cli.download(\"fr_core_news_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htgoVt6HDRBb"},"outputs":[],"source":["import fr_core_news_sm\n","import en_core_web_sm\n","\n","spacy_fr = fr_core_news_sm.load()\n","spacy_en = en_core_web_sm.load()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peqQiDWrE-15"},"outputs":[],"source":["### LATEST TORCHTEXT ###\n","from torchtext.data.utils import get_tokenizer\n","\n","spacy_en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n","spacy_fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzfq9En6Fdlq"},"outputs":[],"source":["### LATEST TORCTHEXT ###\n","\n","from collections import OrderedDict, Counter\n","from torchtext.vocab import vocab\n","import io\n","\n","path = '/content/drive/MyDrive/COLX_531_lab3_jhlbxx/data/'\n","train_fn = 'train_eng_fre.tsv'\n","valid_fn = 'val_eng_fre.tsv'\n","test_fn = 'test_eng_fre.tsv'\n","\n","\n","def build_vocab(filepath, src_tokenizer, trg_tokenizer):\n","  src_counter, trg_counter = Counter(), Counter()\n","  with open(filepath, encoding=\"utf-8\") as f:\n","    for i, line in enumerate(f.readlines()):\n","      if i == 0:  # skip header\n","        continue\n","      # split line and tokenize accordingly\n","      trg_line, src_line = line.strip(\"\\n\").split(\"\\t\")\n","      src_counter.update(src_tokenizer(src_line.lower()))\n","      trg_counter.update(trg_tokenizer(trg_line.lower()))\n","    \n","    # sort and wrap as OrderedDict\n","    # ordered_src = OrderedDict(sorted(src_counter.items(), key=lambda x: x[1], reverse=True))\n","    # ordered_trg = OrderedDict(sorted(trg_counter.items(), key=lambda x: x[1], reverse=True))\n","    ordered_src = sorted(src_counter.items(), key=lambda x: x[1], reverse=True)\n","    ordered_trg = sorted(trg_counter.items(), key=lambda x: x[1], reverse=True)\n","    \n","    # build vocab objects\n","    # NOTE: OrderedDict as input Requires torchtext >= 0.10.0. Using Counter for now\n","    src_vocab = vocab(\n","      src_counter, \n","      min_freq=2, \n","      specials=('<unk>', '<pad>', '<bos>', '<eos>')\n","    )\n","\n","    trg_vocab = vocab(\n","      trg_counter, \n","      min_freq=2,\n","      specials=('<unk>', '<pad>', '<bos>', '<eos>')\n","    )\n","    \n","    return src_vocab, trg_vocab\n","\n","src_vocab, trg_vocab = build_vocab(\n","  path + train_fn, \n","  spacy_fr_tokenizer,\n","  spacy_en_tokenizer\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W__0ihNRPbMt"},"outputs":[],"source":["### LATEST TORCHTEXT ###\n","\n","import io\n","\n","# Define default index to assign to OOV tokens\n","unk_token = '<unk>'\n","src_vocab.set_default_index(src_vocab[unk_token])\n","trg_vocab.set_default_index(trg_vocab[unk_token])\n","\n","def data_process(path, split):\n","  raw_iter = iter(io.open(path + split, encoding=\"utf-8\"))\n","  data = []\n","  for i, item in enumerate(raw_iter):\n","    if i == 0:\n","      continue\n","    trg_raw, src_raw = item.strip(\"\\n\").split(\"\\t\")\n","    src_tensor = torch.tensor(\n","        [src_vocab[token] for token in spacy_fr_tokenizer(src_raw.lower())],\n","        dtype=torch.long\n","      )\n","    trg_tensor = torch.tensor(\n","        [trg_vocab[token] for token in spacy_en_tokenizer(trg_raw.lower())],\n","        dtype=torch.long\n","      )\n","    data.append((src_tensor, trg_tensor))\n","\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDbXDAguRUc1"},"outputs":[],"source":["train_data = data_process(path, train_fn)\n","val_data = data_process(path, valid_fn)\n","test_data = data_process(path, test_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5-wdG4WTbev","outputId":"5ce92cdf-2df7-4133-fb5c-068b10d1ff38","executionInfo":{"status":"ok","timestamp":1678429947469,"user_tz":480,"elapsed":362,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 29000\n","Number of validation examples: 1014\n","Number of testing examples: 1000\n"]}],"source":["print(f\"Number of training examples: {len(train_data)}\")\n","print(f\"Number of validation examples: {len(val_data)}\")\n","print(f\"Number of testing examples: {len(test_data)}\")"]},{"cell_type":"markdown","source":["trg_sent = [trg_vocab.get_itos[i] for i in train_data[0][1]]\n","src_sent = [src_vocab.get_itos[i] for i in train_data[0][0]]\n","print(trg_sent, src_sent)"],"metadata":{"id":"kbGG3FCevVy4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEdP70nHwbyB","outputId":"436a84a8-c1d9-40f2-db02-0ce92505548c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'] ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']\n"]}],"source":["trg_sent = [trg_vocab.itos[i] for i in val_data[100][1]]\n","src_sent = [src_vocab.itos[i] for i in val_data[100][0]]\n","print(trg_sent, src_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcR2kyVbT9dJ","outputId":"35f6e9ac-cb16-45ac-f480-5b1e4d98431d","executionInfo":{"status":"ok","timestamp":1678430037457,"user_tz":480,"elapsed":616,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in source (fr) vocabulary: 6471\n","Unique tokens in target (en) vocabulary: 5893\n"]}],"source":["### LATEST TORCHTEXT ###\n","\n","print(f\"Unique tokens in source (fr) vocabulary: {len(src_vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(trg_vocab)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4maRzpsUJe6","outputId":"3a7bec79-38da-472a-f97a-33c32bd2bc69"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["# trg_stoi = trg_vocab.get_stoi()  # torcthext >0.10.0\n","trg_stoi = trg_vocab.stoi\n","print(trg_stoi['<pad>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHw0elIXvM-S"},"outputs":[],"source":["import pickle\n","\n","with open(\"/content/drive/MyDrive/models/data/trg_vocab\", \"wb\") as f:\n","     pickle.dump(trg_vocab, f)\n","\n","with open(\"/content/drive/MyDrive/models/data/src_vocab\", \"wb\") as f:\n","     pickle.dump(src_vocab, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Att8qScZVetN"},"outputs":[],"source":["### LATEST TORCHTEXT ###\n","\n","BATCH_SIZE = {\n","    \"train\": 16,\n","    \"val\": 256,\n","    \"test\": 256\n","}\n","\n","PAD_IDX = trg_vocab['<pad>']\n","BOS_IDX = trg_vocab['<bos>']\n","EOS_IDX = trg_vocab['<eos>']\n","\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","def generate_batch(data_batch):\n","  src_batch, trg_batch = [], []\n","  for (src_item, trg_item) in data_batch:\n","    src_batch.append(torch.cat([torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0))\n","    trg_batch.append(torch.cat([torch.tensor([BOS_IDX]), trg_item, torch.tensor([EOS_IDX])], dim=0))\n","  src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","  trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n","  return src_batch, trg_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wUuftjPV6GR"},"outputs":[],"source":["train_iter = DataLoader(train_data, batch_size=BATCH_SIZE[\"train\"],\n","                        shuffle=True, collate_fn=generate_batch)\n","valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE[\"val\"],\n","                        shuffle=True, collate_fn=generate_batch)\n","test_iter = DataLoader(test_data, batch_size=BATCH_SIZE[\"test\"],\n","                       shuffle=True, collate_fn=generate_batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmNi0fRKWE9e","outputId":"c51c591c-4844-446c-eafd-f777db5a4c1c","executionInfo":{"status":"ok","timestamp":1678430181131,"user_tz":480,"elapsed":332,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor size of source language: torch.Size([27, 16])\n","tensor size of target language: torch.Size([24, 16])\n","the tensor of first example in target language: tensor([   2,   21,  964,  362, 2688,  202,   96, 2161,  151,   14,    3,    1,\n","           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n"]}],"source":["# batch example of training data\n","for batch in train_iter:\n","    src, trg = batch\n","    print('tensor size of source language:', src.shape)\n","    print('tensor size of target language:', trg.shape)\n","    print('the tensor of first example in target language:', trg[:, 0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"J73SrWVN7nU4"},"source":["## Building the Seq2Seq Model\n","\n","## 2. Encoder\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","First, we'll build the encoder model that encodes the French sentence. We use a single layer `Uni-directional LSTM`.\n","\n","Similar to the classifiction task (covered in DSCI 572), we only pass the output of embedding layer to the LSTM layer. The LSTM layer returns `outputs`, `hidden` and `cell`. The `hidden` is the final hidden state of LSTM layer (t=seq_len). The `cell` is the final cell state of the LSTM layer (t=seq_len). `hidden` and `cell` can be considered as the **context** representation of source language. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY9pEqwL7iKs"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.enc_hid_dim = enc_hid_dim\n","        self.dropout = dropout\n","        self.n_layers = n_layers\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        \n","        #src = [src len, batch size]\n","        \n","        embedded = self.dropout(self.embedding(src))\n","        \n","        #embedded = [src len, batch size, emb dim]\n","        \n","        outputs, (hidden, cell) = self.lstm(embedded)\n","       \n","        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n","        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n","        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n","        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n","        \n","        return hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"gV9fweNEKNcp"},"source":["## 3. Decoder\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","Next up is the decoder. Decoder is a `uni-directional LSTM`.\n","\n","\n","At time step $t$, the input of decoder LSTM is embeded word vector of $t$th target word , $y_t$, the previous decoder hidden state, $h_{t-1}$, and the previous decoder hidden cell, $c_{t-1}$.\n","\n","$$h_t, c_t = \\text{DecoderLSTM}(y_t, (h_{t-1}, c_{t-1}))$$\n","\n","Specially, we will use the last `hidden state` and `cell state` of the encoder LSTM as the initial states of decoder LSTM (i.e., $h_{0}, c_{0}$) rather than randomly initialize them. \n","\n","We then pass hidden state of LSTM layer, $h_t$, through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. \n","\n","$$\\hat{y}_{t+1} = f(h_t)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDL3CtVjKKS3"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.output_dim = output_dim\n","        self.dec_hid_dim = dec_hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, cell):\n","             \n","        # input is of shape [batch_size]\n","        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n","        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n","        \n","        input = input.unsqueeze(0)\n","        \n","        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n","        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n","        \n","        embedded = self.dropout(self.embedding(input))\n","        \n","        #embedded = [1, batch size, emb dim]    \n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        \n","        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n","        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n","        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n","\n","        # sequence_len and num_directions will always be 1 in the decoder.\n","        # output shape is [1, batch_size, hidden_dim]\n","        # hidden shape is [num_layers, batch_size, hidden_dim]\n","        # cell shape is [num_layers, batch_size, hidden_dim]\n","        \n","        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n","        # predicted shape is [batch_size, output_dim]\n","        \n","        return prediction, hidden, cell"]},{"cell_type":"markdown","metadata":{"id":"E5sxYN1dYIrs"},"source":["## 4. Seq2Seq\n","\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","The `encoder` returns both the final `hidden state` and `cell state` to be used as the initial `hidden state` and `cell state` for the `decoder`.\n","\n","Briefly going over all of the steps:\n","- the `outputs` tensor is created to hold all predictions, $\\hat{Y} = \\{\\hat{y_0}, \\hat{y_1} ... \\hat{y_t}\\}$;\n","- the source sequence, $X = \\{x_0,x_1,..., x_t\\}$, is fed into the encoder to receive last hidden state, $h^{Encoder}_t$, and last cell state $c^{Encoder}_t$;\n","- the initial decoder hidden state is set to be the $h^{Encoder}_t$, and the initial decoder cell state is set to be the $c^{Encoder}_t$. (i.e., $h^{Decoder}_0$ = $h^{Encoder}_t$; $c^{Decoder}_0$ = $c^{Encoder}_t$);\n","- we use a batch of `<bos>` tokens as the first `input` (i.e., $y_1$);\n","- we then decode within a loop:\n","\n"," for i in range(1,t): t is the maximal length of target language\n","  - inserting the input token $y_i$, previous hidden state, $h^{Decoder}_{i-1}$, and previous cell state, $c^{Decoder}_{i-1}$, into the decoder;\n","  - receiving a prediction, $\\hat{y}_{i+1}$, which is the most likely output sequence, a new hidden state, $h^{Decoder}_{i}$, and a new cell state, $c^{Decoder}_{i}$;\n","  - we then decide if we are going to **teacher force** or not, setting the next input as appropriate, that is, if teacher forcing is on, the next input will be the gold token from the previous timestep, otherwise, the next input will be the predicted token from the previous timestep."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qu6YDrp0Y2Rc"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    ''' This class contains the implementation of complete sequence to sequence network.\n","    It uses to encoder to produce the context vectors.\n","    It uses the decoder to produce the predicted target sentence.\n","    Args:\n","        encoder: A Encoder class instance.\n","        decoder: A Decoder class instance.\n","    '''\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # src is of shape [src_sequence_len, batch_size]\n","        # trg is of shape [targ_sequence_len, batch_size]\n","        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n","\n","        batch_size = trg.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        # to store the outputs of the decoder\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        # context vector, last hidden and cell state of encoder to initialize the decoder\n","        hidden, cell = self.encoder(src)\n","\n","        # first input to the decoder is the <sos> tokens\n","        input = trg[0, :]\n","\n","        for t in range(1, max_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[t] = output\n","            # pick a random number between 0 to ratio and decide whether to teacher force\n","            # if the ratio is 1.0, use_teacher_force is always 1 \n","            # if the ratio is 0.0, use_teacher_force is always 0\n","            # if the ration is 0.4, use_teacher_force is 1 for 40% of the time (on an average)\n","            use_teacher_force = random.random() < teacher_forcing_ratio \n","            top1 = output.max(1)[1]\n","            # decide the next token based on use_teacher_force]\n","            # if teacher forcing is on, the next input will be the gold token from the previous timestep\n","            # otherwise, the next input will be the predicted token from the previous timestep.\n","            input = (trg[t] if use_teacher_force else top1) \n","\n","        # outputs is of shape [sequence_len, batch_size, output_dim]\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"c3NXkVmygfhv"},"source":["## 5. Training the Seq2Seq Model\n","We instantiate our encoder, decoder and seq2seq model (placing it on the GPU if we have one). "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHsD_I2EzTYA","outputId":"dc424cdb-e1d1-45dd-827c-d6c0eb9342d3","executionInfo":{"status":"ok","timestamp":1678430207814,"user_tz":480,"elapsed":333,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":28}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q_ZqMyitGX-5","outputId":"1410fc82-e452-44f1-9598-be2ca51e4087","executionInfo":{"status":"ok","timestamp":1678430249800,"user_tz":480,"elapsed":3563,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}],"source":["#INPUT_DIM = len(SRC.vocab) # tokens in source vocabulary\n","#OUTPUT_DIM = len(TRG.vocab) # tokens in target vocabulary\n","INPUT_DIM = len(src_vocab) # tokens in source vocabulary\n","OUTPUT_DIM = len(trg_vocab) # tokens in target vocabulary\n","\n","# hyperparameters\n","ENC_EMB_DIM = 256 # encoder embedding size\n","DEC_EMB_DIM = 256 # decoder embedding size\n","ENC_HID_DIM = 512 # encoder hidden size\n","DEC_HID_DIM = 512 # decoder hidden size\n","ENC_DROPOUT = 0.5 # dropout for encoder\n","DEC_DROPOUT = 0.3 # dropout for decoder\n","N_LAYERS = 1 # number of LSTM layers\n","LEARNING_RT = 0.001 # learning rate\n","\n","# model\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n","model = Seq2Seq(enc, dec, device).to(device)"]},{"cell_type":"markdown","metadata":{"id":"jMIV3Tnsh3lU"},"source":["We use a simplified version of the **weight initialization scheme**. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5tsPrp-GZmh","outputId":"867e6e32-5697-435c-df2c-f93d0267fd06","executionInfo":{"status":"ok","timestamp":1678430255374,"user_tz":480,"elapsed":639,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(6471, 256)\n","    (lstm): LSTM(256, 512, dropout=0.5)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (lstm): LSTM(256, 512, dropout=0.3)\n","    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":30}],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","            \n","model.apply(init_weights)"]},{"cell_type":"markdown","metadata":{"id":"6fMQZFQTiCjx"},"source":["Calculate the number of parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9w79-_fiByo","outputId":"c4ed09f7-bd74-4ff1-9dfb-a39206f7e6b8","executionInfo":{"status":"ok","timestamp":1678430273792,"user_tz":480,"elapsed":342,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 9,342,213 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"M9aNXTTZiHts"},"source":["Create an optimizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkdoXq8aiEoF"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"]},{"cell_type":"markdown","metadata":{"id":"xMICykcSiN_k"},"source":["Initialize the loss function. The pad token needs to be ignored."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbjJMMSZiL7e","outputId":"23e2dd9d-219e-4edb-ce48-21f9a91257c0","executionInfo":{"status":"ok","timestamp":1678430283928,"user_tz":480,"elapsed":463,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<pad> token index:  1\n"]}],"source":["#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","print('<pad> token index: ', PAD_IDX)\n","## we will ignore the pad token in true target set\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"]},{"cell_type":"markdown","metadata":{"id":"eB5clLvrieXF"},"source":["### Testing Model With a Single Batch\n","We will run the model with first training batch to test our code."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4yA7zAVBidXt","outputId":"256ff189-1f53-4519-b9d5-2ad29c1fbd20","executionInfo":{"status":"ok","timestamp":1678430295868,"user_tz":480,"elapsed":1561,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.5426, device='cuda:0', grad_fn=<DivBackward0>)\n"]}],"source":["clip = 1\n","model.train()\n","\n","for i, (src, trg) in enumerate(train_iter):\n","    \n","    # read the source sentence and target sentence\n","    # src = batch.SRC\n","    # trg = batch.TRG\n","    src, trg = src.to(device), trg.to(device)\n","\n","    # clear the gradient buffer\n","    optimizer.zero_grad()\n","\n","    # forward pass\n","    output = model(src, trg)\n","    #trg = [trg len, batch size]\n","    #output = [trg len, batch size, output dim]\n","\n","    output_dim = output.shape[-1]\n","\n","    output = output[1:].view(-1, output_dim)\n","    trg = trg[1:].view(-1)\n","\n","    #trg = [(trg len - 1) * batch size]\n","    #output = [(trg len - 1) * batch size, output dim]\n","    \n","    # compute the loss\n","    loss = criterion(output, trg)\n","    \n","    # compute the gradients\n","    loss.backward()\n","\n","    # clip the gradients to prevent gradient explosion problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","    \n","    # update the parameters\n","    optimizer.step()\n","\n","    print(loss/src.shape[1])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"tl_MI5-Hlp60"},"source":["## Fully training process\n","If we test our code successfully. We will start the fully training loop as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dg1-WRQ3lqRA"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, (src, trg) in enumerate(iterator):\n","        \n","        src, trg = src.to(device), trg.to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        # loss function works only 2d logits, 1d targets\n","        # so flatten the trg, output tensors. Ignore the <sos> token\n","        # trg shape shape should be [(sequence_len - 1) * batch_size]\n","        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"XSQfQpQclx1C"},"source":["...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing (i.e., teach forcing rate = 0)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIy-h7i5lwd4"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, (src, trg) in enumerate(iterator):\n","\n","            src, trg = src.to(device), trg.to(device)\n","\n","            output = model(src, trg, 0) # turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"iUeJF_N1NnCD"},"source":["Count the running time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3rv8T7Vly1c"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"gQ8OyS2aNs3_"},"source":["## Training model. \n","\n","We will train the model for 10 epochs. At the end of each epoch, we will save a checkpoint and evaluate on the development set. We will print out the loss and perplexity of train and dev set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaDCA9HOl4XP","outputId":"974812bd-c815-464c-ff94-893b6d870a88","executionInfo":{"status":"ok","timestamp":1678431526009,"user_tz":480,"elapsed":1106422,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 1m 13s\n","\tTrain Loss: 4.663 | Train PPL: 105.992\n","\t Val. Loss: 4.834 |  Val. PPL: 125.754\n","Epoch: 02 | Time: 1m 12s\n","\tTrain Loss: 4.004 | Train PPL:  54.795\n","\t Val. Loss: 4.322 |  Val. PPL:  75.367\n","Epoch: 03 | Time: 1m 13s\n","\tTrain Loss: 3.536 | Train PPL:  34.346\n","\t Val. Loss: 4.072 |  Val. PPL:  58.679\n","Epoch: 04 | Time: 1m 13s\n","\tTrain Loss: 3.171 | Train PPL:  23.827\n","\t Val. Loss: 3.837 |  Val. PPL:  46.403\n","Epoch: 05 | Time: 1m 13s\n","\tTrain Loss: 2.872 | Train PPL:  17.665\n","\t Val. Loss: 3.679 |  Val. PPL:  39.607\n","Epoch: 06 | Time: 1m 14s\n","\tTrain Loss: 2.630 | Train PPL:  13.870\n","\t Val. Loss: 3.599 |  Val. PPL:  36.568\n","Epoch: 07 | Time: 1m 13s\n","\tTrain Loss: 2.428 | Train PPL:  11.341\n","\t Val. Loss: 3.569 |  Val. PPL:  35.470\n","Epoch: 08 | Time: 1m 13s\n","\tTrain Loss: 2.260 | Train PPL:   9.586\n","\t Val. Loss: 3.558 |  Val. PPL:  35.108\n","Epoch: 09 | Time: 1m 13s\n","\tTrain Loss: 2.110 | Train PPL:   8.247\n","\t Val. Loss: 3.532 |  Val. PPL:  34.195\n","Epoch: 10 | Time: 1m 12s\n","\tTrain Loss: 1.982 | Train PPL:   7.260\n","\t Val. Loss: 3.565 |  Val. PPL:  35.338\n","Epoch: 11 | Time: 1m 13s\n","\tTrain Loss: 1.852 | Train PPL:   6.370\n","\t Val. Loss: 3.617 |  Val. PPL:  37.235\n","Epoch: 12 | Time: 1m 13s\n","\tTrain Loss: 1.748 | Train PPL:   5.744\n","\t Val. Loss: 3.684 |  Val. PPL:  39.809\n","Epoch: 13 | Time: 1m 12s\n","\tTrain Loss: 1.649 | Train PPL:   5.204\n","\t Val. Loss: 3.704 |  Val. PPL:  40.602\n","Epoch: 14 | Time: 1m 13s\n","\tTrain Loss: 1.552 | Train PPL:   4.720\n","\t Val. Loss: 3.732 |  Val. PPL:  41.743\n","Epoch: 15 | Time: 1m 12s\n","\tTrain Loss: 1.467 | Train PPL:   4.336\n","\t Val. Loss: 3.749 |  Val. PPL:  42.470\n"]}],"source":["N_EPOCHS = 15\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    # Create checkpoint at end of each epoch\n","    state_dict_model = model.state_dict() \n","    state = {\n","        'epoch': epoch,\n","        'state_dict': state_dict_model,\n","        'optimizer': optimizer.state_dict()\n","        }\n","\n","    torch.save(state, \"/content/drive/MyDrive/models/model_result/uni_LSTM/seq2seq_\"+str(epoch+1)+\".pt\")\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"markdown","metadata":{"id":"X7oPEKhFwIR3"},"source":["## 6. Load Checkpoint\n","We will use the best model for the following process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kAeYOfCwW_U"},"outputs":[],"source":["with open(\"/content/drive/MyDrive/models/data/src_vocab\",\"rb\") as f:\n","     src_vocab = pickle.load(f)\n","\n","with open(\"/content/drive/MyDrive/models/data/trg_vocab\",\"rb\") as f:\n","     trg_vocab = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"yGMLkVAzzLIW"},"source":["Load trained model to `model_best` and put model on device."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCp-dPjIzP3p"},"outputs":[],"source":["# INPUT_DIM = len(SRC_saved.vocab)\n","# OUTPUT_DIM = len(TRG_saved.vocab)\n","INPUT_DIM = len(src_vocab) # tokens in source vocabulary\n","OUTPUT_DIM = len(trg_vocab) # tokens in target vocabulary\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.3\n","N_LAYERS = 1\n","LEARNING_RT = 0.001\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model_best = Seq2Seq(enc, dec, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBmOZKhlzP6x"},"outputs":[],"source":["model_best.load_state_dict(torch.load('/content/drive/MyDrive/models/model_result/uni_LSTM/seq2seq_9.pt')['state_dict'])\n","model_best = model_best.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-BG9snLLiex"},"outputs":[],"source":["# ### INSERT YOUR REFACTORED INFERENCE FUNCTION HERE ###\n","\n","# def inference(model_best, trg_vocab, test_iter, attention=False, max_trg_len = 64):\n","#     ### INSERT REFACTORED CODE HERE ###\n","#     return corpus_bleu_score"]},{"cell_type":"code","source":["def inference(model, trg_vocab, test_iter, attention=False, max_trg_len=64):\n","    from nltk.translate.bleu_score import corpus_bleu\n","\n","    def convert_itos(convert_vocab, token_ids):\n","        list_string = []\n","        for i in token_ids:\n","            if i == convert_vocab.get_stoi()['<eos>']:\n","                break\n","            else:\n","                token = convert_vocab.get_itos()[i]\n","                list_string.append(token)\n","        return list_string\n","\n","    model.eval()\n","    all_trg = []\n","    all_translated_trg = []\n","\n","    TRG_PAD_IDX = trg_vocab['<pad>']\n","\n","    with torch.no_grad():\n","        for i, (src, trg) in enumerate(test_iter):\n","            src, trg = src.to(device), trg.to(device)\n","            batch_size = trg.shape[1]\n","            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n","            trg_placeholder.fill_(TRG_PAD_IDX)\n","            trg_placeholder = trg_placeholder.long().to(device)\n","            if attention:\n","                output, _ = model(src, trg_placeholder, 0)\n","            else:\n","                output = model(src, trg_placeholder, 0)\n","            output_translate = output[1:]\n","            all_trg.append(trg[1:].cpu())\n","\n","            prob, token_id = output_translate.data.topk(1)\n","            translation_token_id = token_id.squeeze(2).cpu()\n","            all_translated_trg.append(translation_token_id)\n","\n","    all_gold_text = []\n","    all_translated_text = []\n","    for i in range(len(all_trg)):\n","        cur_gold = all_trg[i]\n","        cur_translation = all_translated_trg[i]\n","        for j in range(cur_gold.shape[1]):\n","            gold_convered_strings = convert_itos(trg_vocab, cur_gold[:, j])\n","            trans_convered_strings = convert_itos(trg_vocab, cur_translation[:, j])\n","            all_gold_text.append(gold_convered_strings)\n","            all_translated_text.append(trans_convered_strings)\n","\n","    corpus_all_gold_text = [[item] for item in all_gold_text]\n","    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)\n","    return corpus_bleu_score\n"],"metadata":{"id":"qUHbX3AyXhjr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4bx9BX-Liex","executionInfo":{"status":"ok","timestamp":1678432503873,"user_tz":480,"elapsed":78744,"user":{"displayName":"Hao Jia","userId":"07311169520047142158"}},"outputId":"92c98b31-9966-4cfd-e955-2496ce656c43"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.2482460803690209\n"]}],"source":["print(inference(model_best, trg_vocab, test_iter, attention=False, max_trg_len=64))"]},{"cell_type":"markdown","metadata":{"id":"bTBzai_C9tN0"},"source":["## Reference \n","* https://pytorch.org/docs/stable/nn.html\n","* https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n","* https://arxiv.org/abs/1409.3215\n","* https://github.com/graviraja/seq2seq\n","* https://github.com/eladhoffer/seq2seq.pytorch\n","* https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n","* http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n","* https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n","* https://leimao.github.io/blog/Entropy-Perplexity/"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"b7420e893be28a0404d3f16a3a722ac074c4831bbd966aebbe8043ef702d5e1f"}}},"nbformat":4,"nbformat_minor":0}